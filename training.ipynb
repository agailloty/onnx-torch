{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import ImageDataset, compute_acc\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch, torch.nn as nn\n",
    "from torchvision.models import efficientnet_b3\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_folder = 'data/'\n",
    "# Define any image preprocessing steps you want to apply\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create an instance of the dataset\n",
    "train_dataset = ImageDataset(data_folder+'train', transform=transform)\n",
    "val_dataset = ImageDataset(data_folder+'valid', transform=transform)\n",
    "test_dataset = ImageDataset(data_folder+'test', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "class_labels_dict = {v: k for k, v in train_dataset.class_labels.items()}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:250\"\n",
    "print(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 15\n",
    "# Use the dataset with a DataLoader to load the data in batches\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "model = efficientnet_b3(pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the last layer with a custom layer with the number of outputs equal to the number of classes in your dataset\n",
    "num_classes = len(set(train_dataset.labels))\n",
    "model.fc = nn.Linear(in_features=2048, out_features=num_classes).to(device)\n",
    "\n",
    "# to store model parameters during validation (early stopping)\n",
    "best_model = efficientnet_b3(pretrained=True).to(device)\n",
    "best_model.fc = nn.Linear(in_features=2048, out_features=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "show_every = 50\n",
    "\n",
    "# Keep track of the best validation loss\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "patience = 5\n",
    "num_no_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] Training loss: 8.379  Training acc: 0.000\n",
      "Epoch 0: Train Loss: 8.3792, Validation Loss: 8.7388, Train Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "[2,     1] Training loss: 9.133  Training acc: 0.000\n",
      "Epoch 1: Train Loss: 9.1330, Validation Loss: 7.4698, Train Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "[3,     1] Training loss: 7.606  Training acc: 0.000\n",
      "Epoch 2: Train Loss: 7.6063, Validation Loss: 6.7149, Train Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "[4,     1] Training loss: 6.855  Training acc: 0.000\n",
      "Epoch 3: Train Loss: 6.8550, Validation Loss: 6.6123, Train Accuracy: 0.00%, Validation Accuracy: 0.39%\n",
      "[5,     1] Training loss: 6.605  Training acc: 0.000\n",
      "Epoch 4: Train Loss: 6.6049, Validation Loss: 6.5029, Train Accuracy: 0.00%, Validation Accuracy: 0.78%\n",
      "[6,     1] Training loss: 6.542  Training acc: 0.000\n",
      "Epoch 5: Train Loss: 6.5420, Validation Loss: 6.2633, Train Accuracy: 0.00%, Validation Accuracy: 1.57%\n",
      "[7,     1] Training loss: 6.230  Training acc: 0.000\n",
      "Epoch 6: Train Loss: 6.2303, Validation Loss: 5.9721, Train Accuracy: 0.00%, Validation Accuracy: 0.39%\n",
      "[8,     1] Training loss: 5.930  Training acc: 0.000\n",
      "Epoch 7: Train Loss: 5.9300, Validation Loss: 5.6719, Train Accuracy: 0.00%, Validation Accuracy: 0.78%\n",
      "[9,     1] Training loss: 5.755  Training acc: 0.000\n",
      "Epoch 8: Train Loss: 5.7548, Validation Loss: 5.4777, Train Accuracy: 0.00%, Validation Accuracy: 0.78%\n",
      "[10,     1] Training loss: 5.114  Training acc: 0.000\n",
      "Epoch 9: Train Loss: 5.1141, Validation Loss: 5.4495, Train Accuracy: 0.00%, Validation Accuracy: 1.37%\n",
      "[11,     1] Training loss: 4.956  Training acc: 0.000\n",
      "Epoch 10: Train Loss: 4.9564, Validation Loss: 5.5568, Train Accuracy: 0.00%, Validation Accuracy: 1.96%\n",
      "[12,     1] Training loss: 5.487  Training acc: 0.000\n",
      "Epoch 11: Train Loss: 5.4866, Validation Loss: 5.4177, Train Accuracy: 0.00%, Validation Accuracy: 1.76%\n",
      "[13,     1] Training loss: 5.535  Training acc: 0.000\n",
      "Epoch 12: Train Loss: 5.5347, Validation Loss: 5.3168, Train Accuracy: 0.00%, Validation Accuracy: 1.57%\n",
      "[14,     1] Training loss: 4.737  Training acc: 13.333\n",
      "Epoch 13: Train Loss: 4.7372, Validation Loss: 5.2756, Train Accuracy: 13.33%, Validation Accuracy: 1.57%\n",
      "[15,     1] Training loss: 5.209  Training acc: 6.667\n",
      "Epoch 14: Train Loss: 5.2088, Validation Loss: 6.6213, Train Accuracy: 6.67%, Validation Accuracy: 0.98%\n",
      "[16,     1] Training loss: 5.750  Training acc: 6.667\n",
      "Epoch 15: Train Loss: 5.7504, Validation Loss: 5.1817, Train Accuracy: 6.67%, Validation Accuracy: 1.37%\n",
      "[17,     1] Training loss: 5.033  Training acc: 0.000\n",
      "Epoch 16: Train Loss: 5.0331, Validation Loss: 5.1613, Train Accuracy: 0.00%, Validation Accuracy: 0.98%\n",
      "[18,     1] Training loss: 5.607  Training acc: 0.000\n",
      "Epoch 17: Train Loss: 5.6067, Validation Loss: 5.1478, Train Accuracy: 0.00%, Validation Accuracy: 1.57%\n",
      "[19,     1] Training loss: 5.364  Training acc: 0.000\n",
      "Epoch 18: Train Loss: 5.3639, Validation Loss: 5.0900, Train Accuracy: 0.00%, Validation Accuracy: 1.18%\n",
      "[20,     1] Training loss: 5.143  Training acc: 6.667\n",
      "Epoch 19: Train Loss: 5.1433, Validation Loss: 5.0650, Train Accuracy: 6.67%, Validation Accuracy: 1.57%\n",
      "[21,     1] Training loss: 5.230  Training acc: 0.000\n",
      "Epoch 20: Train Loss: 5.2297, Validation Loss: 5.0661, Train Accuracy: 0.00%, Validation Accuracy: 0.98%\n",
      "[22,     1] Training loss: 4.919  Training acc: 0.000\n",
      "Epoch 21: Train Loss: 4.9189, Validation Loss: 5.0427, Train Accuracy: 0.00%, Validation Accuracy: 1.18%\n",
      "[23,     1] Training loss: 4.875  Training acc: 0.000\n",
      "Epoch 22: Train Loss: 4.8755, Validation Loss: 5.0327, Train Accuracy: 0.00%, Validation Accuracy: 0.98%\n",
      "[24,     1] Training loss: 4.845  Training acc: 6.667\n",
      "Epoch 23: Train Loss: 4.8448, Validation Loss: 5.0136, Train Accuracy: 6.67%, Validation Accuracy: 1.37%\n",
      "[25,     1] Training loss: 5.054  Training acc: 0.000\n",
      "Epoch 24: Train Loss: 5.0543, Validation Loss: 5.0146, Train Accuracy: 0.00%, Validation Accuracy: 1.37%\n",
      "[26,     1] Training loss: 5.178  Training acc: 0.000\n",
      "Epoch 25: Train Loss: 5.1785, Validation Loss: 4.9696, Train Accuracy: 0.00%, Validation Accuracy: 2.16%\n",
      "[27,     1] Training loss: 4.690  Training acc: 6.667\n",
      "Epoch 26: Train Loss: 4.6904, Validation Loss: 4.9441, Train Accuracy: 6.67%, Validation Accuracy: 1.18%\n",
      "[28,     1] Training loss: 4.787  Training acc: 6.667\n",
      "Epoch 27: Train Loss: 4.7868, Validation Loss: 4.9605, Train Accuracy: 6.67%, Validation Accuracy: 0.98%\n",
      "[29,     1] Training loss: 4.741  Training acc: 0.000\n",
      "Epoch 28: Train Loss: 4.7414, Validation Loss: 5.0027, Train Accuracy: 0.00%, Validation Accuracy: 0.98%\n",
      "[30,     1] Training loss: 5.010  Training acc: 0.000\n",
      "Epoch 29: Train Loss: 5.0096, Validation Loss: 5.0093, Train Accuracy: 0.00%, Validation Accuracy: 0.98%\n",
      "[31,     1] Training loss: 5.061  Training acc: 0.000\n",
      "Epoch 30: Train Loss: 5.0610, Validation Loss: 4.9481, Train Accuracy: 0.00%, Validation Accuracy: 0.98%\n",
      "[32,     1] Training loss: 4.728  Training acc: 0.000\n",
      "Epoch 31: Train Loss: 4.7278, Validation Loss: 4.9103, Train Accuracy: 0.00%, Validation Accuracy: 1.57%\n",
      "[33,     1] Training loss: 4.577  Training acc: 0.000\n",
      "Epoch 32: Train Loss: 4.5770, Validation Loss: 4.9153, Train Accuracy: 0.00%, Validation Accuracy: 0.98%\n",
      "[34,     1] Training loss: 4.758  Training acc: 0.000\n",
      "Epoch 33: Train Loss: 4.7576, Validation Loss: 4.9139, Train Accuracy: 0.00%, Validation Accuracy: 0.98%\n",
      "[35,     1] Training loss: 4.916  Training acc: 0.000\n",
      "Epoch 34: Train Loss: 4.9163, Validation Loss: 4.8979, Train Accuracy: 0.00%, Validation Accuracy: 1.18%\n",
      "[36,     1] Training loss: 4.732  Training acc: 0.000\n",
      "Epoch 35: Train Loss: 4.7318, Validation Loss: 4.8999, Train Accuracy: 0.00%, Validation Accuracy: 0.98%\n",
      "[37,     1] Training loss: 4.967  Training acc: 0.000\n",
      "Epoch 36: Train Loss: 4.9666, Validation Loss: 4.9092, Train Accuracy: 0.00%, Validation Accuracy: 1.37%\n",
      "[38,     1] Training loss: 5.078  Training acc: 0.000\n",
      "Epoch 37: Train Loss: 5.0779, Validation Loss: 4.9064, Train Accuracy: 0.00%, Validation Accuracy: 1.18%\n",
      "[39,     1] Training loss: 4.834  Training acc: 6.667\n",
      "Epoch 38: Train Loss: 4.8335, Validation Loss: 4.8897, Train Accuracy: 6.67%, Validation Accuracy: 0.98%\n",
      "[40,     1] Training loss: 4.932  Training acc: 0.000\n",
      "Epoch 39: Train Loss: 4.9319, Validation Loss: 4.8851, Train Accuracy: 0.00%, Validation Accuracy: 0.59%\n",
      "[41,     1] Training loss: 4.862  Training acc: 0.000\n",
      "Epoch 40: Train Loss: 4.8617, Validation Loss: 4.8741, Train Accuracy: 0.00%, Validation Accuracy: 0.98%\n",
      "[42,     1] Training loss: 4.827  Training acc: 6.667\n",
      "Epoch 41: Train Loss: 4.8266, Validation Loss: 4.8686, Train Accuracy: 6.67%, Validation Accuracy: 0.98%\n",
      "[43,     1] Training loss: 4.524  Training acc: 0.000\n",
      "Epoch 42: Train Loss: 4.5236, Validation Loss: 4.8888, Train Accuracy: 0.00%, Validation Accuracy: 1.76%\n",
      "[44,     1] Training loss: 4.881  Training acc: 0.000\n",
      "Epoch 43: Train Loss: 4.8811, Validation Loss: 4.9044, Train Accuracy: 0.00%, Validation Accuracy: 1.57%\n",
      "[45,     1] Training loss: 4.767  Training acc: 6.667\n",
      "Epoch 44: Train Loss: 4.7675, Validation Loss: 4.8976, Train Accuracy: 6.67%, Validation Accuracy: 1.57%\n",
      "[46,     1] Training loss: 4.691  Training acc: 13.333\n",
      "Epoch 45: Train Loss: 4.6908, Validation Loss: 4.9025, Train Accuracy: 13.33%, Validation Accuracy: 1.18%\n",
      "[47,     1] Training loss: 5.002  Training acc: 6.667\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(100):  # number of epochs\n",
    "\n",
    "    store_train_loss = []\n",
    "    store_train_acc = []\n",
    "    store_val_loss = []\n",
    "    store_val_acc = []\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_dataloader, 0):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # One-hot encode the labels\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc = compute_acc(outputs, labels)\n",
    "\n",
    "        # Print the statistics\n",
    "        store_train_loss.append(loss.item())\n",
    "        store_train_acc.append(train_acc)\n",
    "\n",
    "        if i % show_every == 0:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] Training loss: %.3f  Training acc: %.3f' % (epoch + 1, i + 1, np.mean(store_train_loss[-show_every:] \n",
    "            ) , np.mean(store_train_acc[-show_every:]) ))\n",
    "        break\n",
    "\n",
    "\n",
    "    # compute epoch loss and accuracy \n",
    "    train_losses.append(np.mean(store_train_loss))\n",
    "    train_accuracies.append(np.mean(store_train_acc))\n",
    "\n",
    "    # Evaluate the model on the validation set \n",
    "    model.eval()\n",
    "\n",
    "    for i, (val_images, val_labels) in enumerate(val_dataloader, 0):\n",
    "        val_images = val_images.to(device)\n",
    "        val_labels = val_labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(val_images)\n",
    "        val_loss = criterion(val_outputs, val_labels)\n",
    "        val_acc = compute_acc(val_outputs, val_labels)\n",
    "\n",
    "        # Print the statistics\n",
    "        store_val_loss.append(val_loss.item())\n",
    "        store_val_acc.append(val_acc)\n",
    "\n",
    "    mean_val_loss = np.mean(store_val_loss)\n",
    "        \n",
    "      #Check if validation loss has improved\n",
    "    if np.mean(mean_val_loss < best_val_loss):\n",
    "        best_val_loss = mean_val_loss\n",
    "        # Get the current state of the model\n",
    "        model_state_dict = model.state_dict()\n",
    "        best_model.load_state_dict(model_state_dict)\n",
    "        num_no_improvement = 0 \n",
    "    else:\n",
    "        num_no_improvement+=1 \n",
    "    if num_no_improvement == patience: \n",
    "        break\n",
    "\n",
    "    # compute epoch loss and accuracy \n",
    "    val_losses.append(np.mean(store_val_loss))\n",
    "    val_accuracies.append(np.mean(store_val_acc))\n",
    "\n",
    "    # Print loss and acc at the end of the epoch\n",
    "    print(\"Epoch {}: Train Loss: {:.4f}, Validation Loss: {:.4f}, Train Accuracy: {:.2f}%, Validation Accuracy: {:.2f}%\".format\n",
    "    (epoch, train_losses[-1], val_losses[-1], train_accuracies[-1], val_accuracies[-1]))\n",
    "\n",
    "   \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is:  0.9803921568627452\n"
     ]
    }
   ],
   "source": [
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "store_test_acc = []\n",
    "\n",
    "for i, (test_images, test_labels) in enumerate(test_dataloader, 0):\n",
    "    \n",
    "    test_images = test_images.to(device)\n",
    "    test_labels = test_labels.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(test_images)\n",
    "    test_acc = compute_acc(test_outputs, test_labels)\n",
    "\n",
    "    store_test_acc.append(test_acc)\n",
    "\n",
    "avg_test_accuracy = np.mean(store_test_acc)\n",
    "print(\"The test accuracy is: \", avg_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path = Path(os.path.dirname(\".\")) / \"bestmodel.pt\"\n",
    "torch.save(best_model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(BATCH_SIZE, \u001b[39m3\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m torch_out \u001b[39m=\u001b[39m best_model(x)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Export the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m torch\u001b[39m.\u001b[39monnx\u001b[39m.\u001b[39mexport(torch_out,               \u001b[39m# model being run\u001b[39;00m\n\u001b[0;32m      6\u001b[0m                   x,                         \u001b[39m# model input (or a tuple for multiple inputs)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mefficientb3.onnx\u001b[39m\u001b[39m\"\u001b[39m,   \u001b[39m# where to save the model (can be a file or file-like object)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m                   dynamic_axes\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m : {\u001b[39m0\u001b[39m : \u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m},    \u001b[39m# variable length axes\u001b[39;00m\n\u001b[0;32m     14\u001b[0m                                 \u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m : {\u001b[39m0\u001b[39m : \u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m}})\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\efficientnet.py:355\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\efficientnet.py:345\u001b[0m, in \u001b[0;36mEfficientNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 345\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[0;32m    347\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m    348\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "x = torch.randn(BATCH_SIZE, 3, 224, 224, requires_grad=True)\n",
    "torch_out = best_model(x)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(torch_out,               # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"efficientb3.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6db2167fb4fcacd03f8bd02d15ab4c3ca7c480041d518a2530fbd4947c1ca8e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
